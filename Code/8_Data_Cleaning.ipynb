{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-Data_Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM8ePIYVoS2iPjGjDN44Wfb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCda_7rUwmds"
      },
      "source": [
        "**Import the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzeTeN_3cUAz"
      },
      "source": [
        "pip install pyarabic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMt_SqYDig8V"
      },
      "source": [
        "#-- not all the imported lib are used in this notebook --#\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "from nltk.corpus import stopwords\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#Import arabic libraries\n",
        "import pyarabic.araby as araby\n",
        "import pyarabic.number as number\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPIVPDdI7qyR"
      },
      "source": [
        "#Part1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x84AImCFXwx"
      },
      "source": [
        "1-Removing the dublicates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKsqpop9Ajtb"
      },
      "source": [
        "#df=pd.read_csv('/content/in-class8.csv',usecols=['date','username','name','tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Hz45yLWw1y"
      },
      "source": [
        "df=pd.read_excel('/content/ALL.xlsx',usecols=['date','username','name','tweet','events'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17fa28pgIaYx"
      },
      "source": [
        "#dublicates in tweet column\n",
        "df.tweet.duplicated().sum()#we can remove the \"sum\" to see what columns that are duplicated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRS0N5qoJp_T"
      },
      "source": [
        "df.duplicated().sum()#to see dublicates in general not only specific column "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHchLKqrKNnP"
      },
      "source": [
        "df.loc[df.duplicated(keep='last'), :]# to see what are these columns - 'first' can also be changed to last depends on what cloumns you want to consider as first appeared or use False "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvZohDdfM85i"
      },
      "source": [
        "#df.duplicated(subset=['tweet','name']).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCei9s86IoTV"
      },
      "source": [
        "#df.tweet.drop_duplicates(keep='first').shape # 'first' 'last' False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL_uoxeMA795"
      },
      "source": [
        "print(df[~df.duplicated(subset=['tweet'])]) # the ~ sign is to remove the dublicated fields! ,'name'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l5_d_cTE07A"
      },
      "source": [
        "df[~df.duplicated(subset=['tweet'])].to_csv('8_dropduplicated.csv',index=False)#save the cleaned data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwadETInrpoN"
      },
      "source": [
        "2-Drop rows with certine value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZwnDNlXsJqi"
      },
      "source": [
        "df=pd.read_excel('/content/ALL_p3.xlsx',usecols=['date','username','name','tweet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hbNcKQaygN3"
      },
      "source": [
        "#Drop the row if the \"name\"  contains any keyword mentioned in \"searchfor\"\n",
        "searchfor = ['Ù…Ø¯ÙŠÙ†Ø©','Ø¯Ù„ÙŠÙ„','ØºØ±ÙØ©','ØªØ£Ø¬ÙŠØ±','Ø¨Ù„Ø¯ÙŠØ©','ÙƒÙ„ÙŠØ©','Ø£Ù†Ø¨Ø§Ø¡','Ù…ÙˆÙ‚Ø¹','Ù…Ù†ÙˆØ¹Ø§Øª','ØªØ¹Ù„ÙŠÙ…','Ù…Ø¨Ø§Ø¯Ø±Ø©','ÙˆØ¸ÙŠÙØ©','Ù…Ø¬Ù…Ø¹','Ø§Ù„Ø£Ù†','ÙˆØ§ØªØ³','Ù…Ø´Ø§Ù‡Ø¯','Ù…ÙˆØ¬Ø²','ØªØ­Ø¯ÙŠØ«','ÙˆØ³Ù…','Ø§Ù„ÙƒØ±Ø©','Ø³Ù†Ø§Ø¨','Ù…Ø­Ø§ÙØ¸Ø©','Ø¥Ù…Ø§Ø±Ø©','Ø´Ø±ÙƒØ©','Ø¹ÙƒØ§Ø¸','Ø§Ù„Ø§Ù‡Ø±Ø§Ù…','Ù‡Ø§Ø´ØªØ§Ù‚','Ø§Ù„ÙŠÙˆÙ…','Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±ÙŠØ©','Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ©','Ø¥Ø¬Ø§Ø²Ø©','Ø§Ø¬Ø§Ø²Ø©','Ø¥Ø¯Ø§Ø±Ø©','Ø±ØªÙˆÙŠØª','Ø¹Ù†Ø§ÙˆÙŠÙ†','Ø¥Ø°Ø§Ø¹Ø©','channel','news','Ù…Ø¹Ù‡Ø¯','Ø¬Ø§Ù…Ø¹Ø©','Ù…Ø¯Ø±Ø³Ø©','Ø­Ù‚Ø§Ø¦Ù‚','Ø¯Ù„ÙŠÙ„','Ø§Ù„ÙˆØ·Ù†','Ø§ÙˆÙ† Ù„Ø§ÙŠÙ†','Ù…Ø­ØªÙˆÙ‰','Ø®Ø·Ø§Ø¨Ø©','Ø§Ù„ÙŠÙ…Ù†ÙŠØ©','Ø§Ù„Ø³ÙˆØ±ÙŠØ©','Ø§Ù„Ø§Ø±Ø¯Ù†ÙŠØ©','Ø§Ù„ÙƒÙˆÙŠØªÙŠØ©','Ø§Ù„Ø§Ù…Ø§Ø±Ø§ØªÙŠØ©','Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©','Ø§Ù„Ø¯ÙˆÙ„ÙŠØ©','Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ©','Ø´Ø¨ÙƒØ©','Ø§ÙˆÙ†Ù„Ø§ÙŠÙ†','Ù†ÙŠÙˆØ²','ØµØ­ÙŠÙØ©', 'Ø¬Ø±ÙŠØ¯Ø©' ,'Ù‚Ù†Ø§Ø© ','Ù…Ø¬Ù„Ø©','Ø¬ÙˆØ§Ù„','Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©' ,'Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©','Ù…ÙˆØ§Ø¹ÙŠØ¯','Ø­Ø¬ÙˆØ²Ø§Øª','Ø­Ø¬Ø²','Ø£Ø®Ø¨Ø§Ø±','Ø§Ø®Ø¨Ø§Ø±']\n",
        "data = df[~df.name.str.contains('|'.join(searchfor), na=False)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvDo6TCmR6xV"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR1V_4dMefWh"
      },
      "source": [
        "data = df[~df.name.str.contains('|'.join(searchfor), na=False)].to_excel('drop1.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIXNyGgcSeqj"
      },
      "source": [
        "df=pd.read_excel('/content/drop1.xlsx',usecols=['date','username','name','tweet','events'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw__d3BJ3-x2"
      },
      "source": [
        "#Drop the row if the \"name\"  contains any keyword from list1 except it has any keyword from list2 in \"tweet\" then keep it.\n",
        "List1 = ['_q8','ðŸ‡¶ðŸ‡¦','ðŸ‡°ðŸ‡¼','Q8','uae','kuwait','leban','syri','qata','qtri','lib','yem','oman','Kuwaiti','egy','Egyp','egypt','q8','ðŸ‡ªðŸ‡¬','rtarabic','ðŸ‡§ðŸ‡­','iraq','ðŸ‡µðŸ‡¸','online','ðŸ‡¦ðŸ‡ª']\n",
        "List2 = ['ksa','Saudi','Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©','ØªÙˆÙƒÙ„Ù†Ø§','Tawakkalna','Ø§Ù„Ù…Ù…Ù„ÙƒØ©']\n",
        "data1=data[~data[\"name\"].astype(str).str.contains('|'.join(List1)) | data[\"tweet\"].astype(str).str.contains('|'.join(List2))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ytoGyhuTedm"
      },
      "source": [
        "len(data1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-cp3JC3hsVI"
      },
      "source": [
        "#Drop the row if the \"username\"  contains any keyword from list1 except it has any keyword from list2 in \"tweet\" then keep it.\n",
        "List1 = ['_q8','q8','Q8','kw','uae','kuwait','leban','syri','qata','qtri','lib','yem','oman','Kuwaiti','egy','egypt','rtarabic','bh','iraq','online']\n",
        "List2 = ['ksa','Saudi','Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©','ØªÙˆÙƒÙ„Ù†Ø§','Tawakkalna','Ø§Ù„Ù…Ù…Ù„ÙƒØ©']\n",
        "data2= data1[~data1[\"username\"].astype(str).str.contains('|'.join(List1)) | data1[\"tweet\"].astype(str).str.contains('|'.join(List2))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVE0_UYCTa0W"
      },
      "source": [
        "len(data2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEsVVvcIhdpz"
      },
      "source": [
        "data2.to_excel('multipleq_clean_part3.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fNLVHSO7gQK"
      },
      "source": [
        "#Part2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSdw9MyHi1AZ"
      },
      "source": [
        "df=pd.read_excel('/content/ALL.xlsx', usecols=['date','tweet'])#Reading the data file and show only tweet column\n",
        "df.head()#show the file content\n",
        "#,'username','name'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDKnndhVcZWD"
      },
      "source": [
        "#df.loc[df['tweet'].str.contains('@', case=False), 'tweet'] = '@username'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzs-vEPZDmmp"
      },
      "source": [
        "Remove Dicritics and normalization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgIVDlHJjM4p"
      },
      "source": [
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"[Ø¥Ø£Ø¢]\", \"Ø§\", text)\n",
        "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
        "    text = re.sub(\"Ø¤\", \"Ùˆ\", text)\n",
        "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
        "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
        "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
        "    return text;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe8lluP3jR6U"
      },
      "source": [
        "df[\"tweet\"]=df[\"tweet\"].apply(lambda x : normalize_arabic(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7h_VudwjgIQ"
      },
      "source": [
        "print(df)#no need but i like to see the result step by step!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWQry0W1jJ3S"
      },
      "source": [
        "\n",
        "\n",
        "arabic_diacritics = re.compile(\"\"\" Ù‘    | # Tashdid\n",
        "                             ÙŽ    | # Fatha\n",
        "                             Ù‹    | # Tanwin Fath\n",
        "                             Ù    | # Damma\n",
        "                             ÙŒ    | # Tanwin Damm\n",
        "                             Ù    | # Kasra\n",
        "                             Ù    | # Tanwin Kasr\n",
        "                             Ù’    | # Sukun\n",
        "                             Ù€     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89HjIMqRjYPl"
      },
      "source": [
        "def remove_diacritics(text):\n",
        "    text = re.sub(arabic_diacritics,\"\", text)\n",
        "    return text;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID2fy3g1ja_B"
      },
      "source": [
        "  df[\"tweet\"]=df[\"tweet\"].apply(lambda x : remove_diacritics(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I073LEQEklUx"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsW2_mRbsJi2"
      },
      "source": [
        "# this code removes all words contains underscore from the dataset\n",
        "def filter_string(str):\n",
        "  a=[]\n",
        "  for i in str.split():\n",
        "        if \"_\" not in i:\n",
        "            a.append(i)\n",
        "  return ' '.join(a)\n",
        "\n",
        "df[\"tweet\"]=df[\"tweet\"].apply(lambda x : filter_string(x) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uDQghRPksw9"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2a-JXtNw3UZ"
      },
      "source": [
        "\n",
        "\n",
        "> **clean data from:**\n",
        "\n",
        "\n",
        "* Numbers\n",
        "*Upper and lower case english letters\n",
        "*Newline "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJbsuKKZbLuE"
      },
      "source": [
        "#arabic_numbers = 'Û°Û±Û²Û³Ù¤Ù¥Ù¦Ù§Û¸Û¹'\n",
        "def clean_tweets(text):\n",
        " \n",
        " text=re.sub(\"[0-9]\",\"\",text)#Remove Numbers\n",
        " text=re.sub(\"[a-zA-Z]\",\"\",text)#all english letters\n",
        " text=re.sub(\"_\",\" \",text) #Already take care of in the previous step!\n",
        " text=re.sub(\"\\n\",\"\",text)#remove newline\n",
        " #text=re.sub(\"[+arabic_numbers+]\",\"\",text)\n",
        " return text;\n",
        "\n",
        "#arabic_numbers = 'Û°Û±Û²Û³Ù¤Ù¥Ù¦Ù§Û¸Û¹'\n",
        "#df['tweet']= df['tweet'].str.replace(\"[\"+arabic_numbers+\"]\",'')\n",
        "#df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV8Sb5kLcDxR"
      },
      "source": [
        "  df[\"tweet\"]=df[\"tweet\"].apply(lambda x : clean_tweets(x) )# save the result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlpVEnFwcLJC"
      },
      "source": [
        "  print(df)# show the result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt84xtckjvoW"
      },
      "source": [
        "df['tweet'] = df['tweet'].str.replace(r'\\W',\" \")#Remove Special \"Char\" after removing the diacritics to avoid deleteing the text contains diacritics\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qF58vkilBqo"
      },
      "source": [
        "df['tweet'] = df['tweet'].str.replace(r' +',\" \") #Remove more than one space betweeen the words\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jv5B9ycmkyp"
      },
      "source": [
        "#df=pd.read_csv('/content/processed_new_vaccine.csv')\n",
        "df.to_excel(\"/content/ALL_clean.xlsx\")# save in excel format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-ou_zATFg72"
      },
      "source": [
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Wmvd6TRC--5"
      },
      "source": [
        "df=pd.read_excel('/content/all datacollected.xlsx', usecols=['sentiment','tweet'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rpbyj1QDyh0"
      },
      "source": [
        "df['sentiment'] = df.sentiment.map({'neutral':2, 'negative':0, 'positive':1})\n",
        "print(df.shape)# to get the size\n",
        "df.head() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}